# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /env: deegan

image_size: [396, 640]
point_cloud_range: [-51.2, -51.2, -10, 51.2, 51.2, 10]
voxel_size: [0.2, 0.2, 20]
batch_size: 1
num_workers: 4
pkl_cfg_file: dataset_config_clean_seperation_subsample_10.pkl
use_images: true
use_lidar: true
use_raw_elevation: true
use_gvom: false

general:
  name: "refactor/trial"
  dataset_dir: ${env.dataset_root_dir}
  result_dir: ${env.result_dir}
  model_path: ${general.result_dir}/${general.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
  tags: ["refactor"]
  train: true   # set false to skip model training
  test: true
  ckpt_path: /home/jonfrey/trained_models/camp_roberts_refactored/2024-01-02_20-39-14/last.ckpt # simply provide checkpoint path to resume training
  seed: 41 # seed for random number generators in pytorch, numpy and python.random

trainer:
  _target_: lightning.pytorch.trainer.Trainer
  precision: 16-mixed
  accumulate_grad_batches: 1
  fast_dev_run: false
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  max_epochs: -1
  max_steps: 28000
  num_sanity_val_steps:  0
  check_val_every_n_epoch: 1
  accelerator: gpu
  detect_anomaly: false
  log_every_n_steps: 10
  profiler: false  # 
  devices: 1  # All
  deterministic: false
  # strategy: str = None  # ddp # TODO

model:
  _target_: perception_bev_learning.lightning.LightningBEV
  path: ${general.model_path}

  target_layers:
    wheel_risk: 
      name: wheel_risk
      loss_scale: 2.0
      pre_loss_function:
        _target_: torch.sigmoid
        _partial_: true
      loss_function:
        _target_: perception_bev_learning.loss.mse_regression
        _partial_: true
        bin_low: 0
        bin_high: 1
        weight: null
      post_loss_function:
        _target_: perception_bev_learning.loss.skip
        _partial_: true
    elevation: 
      name: elevation
      loss_scale: 1.0
      pre_loss_function:
        _target_: perception_bev_learning.loss.skip
        _partial_: true
      loss_function:
        _target_: perception_bev_learning.loss.mse_regression
        _partial_: true
        bin_low: -1
        bin_high: 1
        weight: null
      post_loss_function:
        _target_: perception_bev_learning.loss.skip
        _partial_: true

  visualizer:
    # Plotting Configuration
    path: ${general.model_path}/visualize
    train: 0
    val: 0
    test: -1
    plot_sequence: false
    tag_by_dataloader: true
    tag_with_front_camera: false
    # Plotting BEV Maps
    plot_all_risks: true
    plot_all_elevations: true
    plot_all_maps: false
    plot_pcd_bev: false
    # Plotting Images
    plot_raw_images: false
    # Plotting Project data onto images
    project_gt_BEV_on_image: false
    project_pred_BEV_on_image: false
    project_cams: [1, 0, 2, 3]
    project_pcd_on_image: false
    # Generate Summary Dashboard
    plot_dashboard: false

  network:
    # specify the main network module (BevTravNet in our case)
    _target_: perception_bev_learning.models.fusion_models.bev_trav_net.BevTravNet
    cfg_model:
      use_images: true
      use_lidar: true
      use_raw_elevation: true
      use_gvom: true
      use_seperate_decoder: false # TODO
      encoders:
        camera:
          backbone:
            type: SwinTransformer
            embed_dims: 96
            depths: [2, 2, 6, 2]
            num_heads: [3, 6, 12, 24]
            window_size: 7
            mlp_ratio: 4
            qkv_bias: true
            qk_scale: null
            drop_rate: 0.
            attn_drop_rate: 0.
            drop_path_rate: 0.3
            patch_norm: true
            out_indices: [1, 2, 3]
            with_cp: false
            convert_weights: true
            init_cfg:
              type: Pretrained
              checkpoint: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth
          neck:
            type: GeneralizedLSSFPN
            in_channels: [192, 384, 768]
            out_channels: 256
            start_level: 0
            num_outs: 3
            norm_cfg:
              type: BN2d
              requires_grad: true
            act_cfg:
              type: ReLU
              inplace: true
            upsample_cfg:
              mode: bilinear
              align_corners: false
          vtransform:
            type: LSSTransform
            in_channels: 256
            out_channels: 80
            image_size: ${image_size}
            # feature_size: ${image_size}//8 #TODO
            feature_downsample: 8
            xbound: [-51.2, 51.2, 0.2]
            ybound: [-51.2, 51.2, 0.2]
            zbound: [-20.0, 20.0, 40.0]
            dbound: [1.0, 60.0, 0.5]
            downsample: 2
        lidar:
          voxelize:
            max_num_points: 16
            point_cloud_range: ${point_cloud_range}
            voxel_size: ${voxel_size}
            max_voxels: [32000, 64000]
          voxelize_reduce: false
          backbone:
            type: PointPillarsNet
            voxel_encoder:
              type: PillarFeatureNet
              in_channels: 3 # (x, y, z)
              feat_channels: [64]
              with_distance: False
              voxel_size: ${voxel_size}
              point_cloud_range: ${point_cloud_range}
            middle_encoder:
              type: PointPillarsScatter
              in_channels: 64
              output_shape: [512, 512] # (y_max-y_min / y_res, x_max-x_min / x_res)
            backbone:
              type: SECOND
              in_channels: 64
              out_channels: [64, 128, 256]
              layer_nums: [3, 5, 5]
              layer_strides: [2, 2, 2]
            neck:
              type: SECONDFPN
              in_channels: [64, 128, 256]
              out_channels: [32, 32, 32]
              upsample_strides: [1, 2, 4]
              use_conv_for_no_stride: true
      fuser:
        type: ConvFuser
        in_channels: [80, 96]
        out_channels: 256
      decoder:
        backbone:
          type: SECOND
          in_channels: 256
          out_channels: [128, 256]
          layer_nums: [5, 5]
          layer_strides: [1, 2]
          norm_cfg:
            type: BN
            eps: 1.0e-3
            momentum: 0.01
          conv_cfg:
            type: Conv2d
            bias: false
        neck:
          type: SECONDFPN
          in_channels: [128, 256]
          out_channels: [256, 256]
          upsample_strides: [1, 2]
          norm_cfg:
            type: BN
            eps: 1.0e-3
            momentum: 0.01
          upsample_cfg:
            type: deconv
            bias: false
          use_conv_for_no_stride: true
      head:
        type: BEVSimpleHead
        in_channels: 512
        channels_wheel_risk: 1
        channels_elevation: 1
        scale_factor: 2

  optimizer:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 1.0e-3
    weight_decay: 1.0e-7

  scheduler:
    _target_: torch.optim.lr_scheduler.OneCycleLR
    _partial_: true
    max_lr: ${model.optimizer.lr}
    total_steps: ${trainer.max_steps}
    pct_start: 0.1
    div_factor: 25.0
    final_div_factor: 10000.0
