# @package _global_

defaults:
  - _self_
  - env: deegan
  - logger: wandb
  - experiment: null

image_size: [396, 640]
point_cloud_range: [-51.2, -51.2, -25, 51.2, 51.2, 25]
upsample_interpolate_neck: 2

voxel_size: [0.8, 0.8, 50]
xbound: [-51.2, 51.2, 0.8]
ybound: [-51.2, 51.2, 0.8]
zbound: [-25.0, 25.0, 50.0]
dbound: [1.0, 60, 0.5]
pointpillar_output_shape: [128, 128] # (x_max-x_min / x_res, y_max-y_min / y_res)
upsample_after_conv: 1
downsample_raw_ele: 4
name: cr_d2f/128_micro_debug

return_n_pointclouds: 1
pointcloud_merge_threshold: 2
voxel_downsample: false
voxel_downsample_size: 0.4

batch_size: 1
num_workers: 2
training_steps: 16000
lr: 5e-4

persistent_workers: false
prefetch_factor: 2
pkl_cfg_file: dataset_config_clean_seperation_subsample_1.pkl
use_images: true
use_lidar: true
use_raw_elevation: true
use_minz_pcl: false
use_gvom: false
use_minz_gvom: false
use_gvom_semantics: false

target_shape_micro: [512, 512]
gmr_resolution: 0.8
elevation_rescale: 0.04

freeze_backbones: false
sequence_length: 1
bptt_sequence_length: 1
use_temporal_fusion: false
test_full_sequence: false

general:
  name: ${name}
  dataset_dir: ${env.dataset_root_dir}
  result_dir: ${env.result_dir}
  model_path: ${general.result_dir}/${general.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
  tags: ["refactor"]
  train: true   # set false to skip model training
  test: true
  ckpt_path: null # simply provide checkpoint path to resume training
  seed: 45 # seed for random number generators in pytorch, numpy and python.random

hydra:
  run:
    dir: ${general.model_path}/hydra

logger:
  wandb:
    name: ${general.name}
    entity: "manthan99"
    save_dir: ${general.result_dir}
    project: "bev_multi"

datamodule:
  _target_: perception_bev_learning.dataset.BEVDataModuleMulti
  dataloader:
    train_dataloader:
      batch_size: ${batch_size}
      num_workers: ${num_workers}
      persistent_workers: ${persistent_workers}
      prefetch_factor: ${prefetch_factor}
      shuffle: true
      drop_last: true
      pin_memory: true
    val_dataloader:
      batch_size: ${batch_size}
      num_workers: ${num_workers}
      persistent_workers: ${persistent_workers}
      prefetch_factor: ${prefetch_factor}
      shuffle: false
      drop_last: false
      pin_memory: true
    test_dataloader:
      batch_size: ${batch_size}
      num_workers: ${num_workers}
      persistent_workers: ${persistent_workers}
      prefetch_factor: ${prefetch_factor}
      shuffle: false
      drop_last: false
      pin_memory: true

  dataset:
    root_dir: ${general.dataset_dir}
    cfg_file: ${pkl_cfg_file}
    sequence_length: ${sequence_length}
    bptt_sequence_length: ${bptt_sequence_length}
    mode: train
    return_n_pointclouds: ${return_n_pointclouds}
    pointcloud_merge_threshold: ${pointcloud_merge_threshold} # in meters
    voxel_downsample: ${voxel_downsample}
    voxel_downsample_size: ${voxel_downsample_size}
    return_gvom_cloud: true
    return_image: true
    deterministic_shuffle: false
    enable_profiling: false
    return_test: true
    image_keys: [multisense_front, multisense_left, multisense_right, multisense_back]
    depth_keys: [multisense_front_depth, multisense_left_depth, multisense_right_depth, multisense_back_depth]
    travmap_gt_key: g_traversability_map_micro_gt # TODO Remove
    travmap_key: g_traversability_map_micro # TODO Remove
    elemap_key: g_raw_ele_map_micro # TODO Remove
    anchor_key: multisense_front
    pointcloud_key: velodyne_merged_points
    gvom_key: pointcloud_map-points_micro
    use_gvom_semantics: ${use_gvom_semantics}

    target_layers:
      micro:
        shape: ${target_shape_micro}
        layers:
          wheel_risk:
            name: wheel_risk
            gridmap_topic: g_traversability_map_micro_gt
            scale: 1.0
            clip_min: 0
            clip_max: 1
          elevation:
            name: elevation_dem
            gridmap_topic: g_elevation_map_micro_dem
            scale: ${elevation_rescale}
            clip_min: -1
            clip_max: 1

    aux_layers:
      micro:
        shape: ${target_shape_micro}
        layers:
          wheel_risk:
            name: wheel_risk
            gridmap_topic: g_traversability_map_micro
            clip_min: 0
            clip_max: 1
            scale: 1.0
          elevation_raw:
            name: elevation_raw
            gridmap_topic: g_raw_ele_map_micro
            scale: ${elevation_rescale}
            clip_min: -1
            clip_max: 1
          elevation:
            name: elevation
            gridmap_topic: g_traversability_map_micro
            scale: ${elevation_rescale}
            clip_min: -1
            clip_max: 1
          reliable:
            name: reliable
            gridmap_topic: g_traversability_map_micro
            clip_min: 0
            clip_max: 1
            scale: 1.0
          reliable_gt:
            name: reliable
            gridmap_topic: g_traversability_map_micro_gt
            clip_min: 0
            clip_max: 1
            scale: 1.0
          confidence_gt:
            name: confidence
            gridmap_topic: g_traversability_map_micro_gt
            clip_min: 0
            clip_max: 1
            scale: 1.0
      
    img_augmentation:
      H: ${image_size[0]}
      W: ${image_size[1]}
      fH: ${image_size[0]}
      fW: ${image_size[1]}
      resize_lim: [0.8, 1.0] # this should be roughly fH/H or fW/W
      bot_pct_lim: [-0.05, 0.05] # percentage of scaled image to crop
      rot_lim: [-5.4, 5.4]
      rand_flip: false

callbacks:
  model_checkpoint_step:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    save_last: true
    filename: "{epoch}-{step}---last"
    dirpath: ${general.model_path}
  model_checkpoint_elevation:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    save_top_k: 1
    save_last: false
    monitor: "val/micro/elevation/Pred/Total_MAE"
    filename: "{epoch}-{step}---{val/micro/elevation/Pred/Total_MAE:.3f}"
    dirpath: ${general.model_path}
  model_checkpoint_risk:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    save_top_k: 1
    save_last: false
    monitor: "val/micro/wheel_risk/Pred/Total_MSE"
    filename: "{epoch}-{step}---{val/micro/wheel_risk/Pred/Total_MSE:.3f}"
    dirpath: ${general.model_path}
  model_summary:
    _target_: lightning.pytorch.callbacks.RichModelSummary
    max_depth: -1
  learning_rate:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step
  rich_progress_bar:
    _target_: lightning.pytorch.callbacks.RichProgressBar

trainer:
  _target_: lightning.pytorch.trainer.Trainer
  # precision: 16-mixed
  accumulate_grad_batches: 1
  fast_dev_run: false
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  max_epochs: -1
  max_steps: ${training_steps}
  num_sanity_val_steps:  0
  check_val_every_n_epoch: 1
  accelerator: gpu
  detect_anomaly: false
  profiler: false  # 
  devices: 1  # All
  deterministic: false
  # strategy: str = None  # ddp # TODO

model:
  _target_: perception_bev_learning.lightning.LightningBEVMulti
  path: ${general.model_path}
  test_full_sequence: ${test_full_sequence} 
  sequence_length: ${sequence_length}
  batch_size: ${batch_size}

  target_layers:
    micro:
      wheel_risk: 
        name: wheel_risk
        loss_scale: 2.0
        channels: 1
        pre_loss_function:
          _target_: torch.sigmoid
          _partial_: true
        loss_function:
          _target_: perception_bev_learning.loss.wheel_risk_mse
          _partial_: true
        post_loss_function:
          _target_: perception_bev_learning.loss.skip
          _partial_: true
      elevation: 
        name: elevation
        loss_scale: 2.0
        channels: 1
        pre_loss_function:
          _target_: perception_bev_learning.loss.skip
          _partial_: true
        loss_function:
          _target_: perception_bev_learning.loss.elevation_fused_l1
          _partial_: true
        post_loss_function:
          _target_: perception_bev_learning.loss.skip
          _partial_: true
   
  metrics:
    mse: true  # Compute MSE for all target layers
    mae: true  # Compute MAE for all target layers
    range_metrics: true
    observed_vs_unobserved: true
    hdd_statistic: true
    fatal_risk: 0.35
    target_layers: ${datamodule.dataset.target_layers} 
    aux_layers: ${datamodule.dataset.aux_layers} 
    modes: ["train", "val", "test"]

  visualizer:
    # Plotting Configuration
    path: ${general.model_path}/visualize
    train: 0
    val: 0
    test: 0
    plot_sequence: false
    tag_by_dataloader: true
    tag_with_front_camera: false
    # Plotting BEV Maps
    plot_all_risks: false
    plot_all_elevations: false
    plot_all_maps: true
    plot_pcd_bev: false
    # Plotting Images
    plot_raw_images: true
    # Plotting Project data onto images
    project_gt_BEV_on_image: true
    project_pred_BEV_on_image: true
    project_cams: [1, 0, 2, 3]
    project_pcd_on_image: false
    # Generate Summary Dashboard
    plot_dashboard: true

  network:
    # specify the main network module (BevTravNet in our case)
    _target_: perception_bev_learning.models.fusion_models.BevTravNetMulti
    cfg_model:
      use_images: ${use_images}
      use_lidar: ${use_lidar}
      use_raw_elevation: ${use_raw_elevation}
      use_minz_pcl: ${use_minz_pcl}
      aux_layers: ${datamodule.dataset.aux_layers}
      use_gvom: ${use_gvom}
      use_minz_gvom: ${use_minz_gvom}
      gmr_resolution: ${gmr_resolution}
      sequence_length: ${sequence_length}
      use_temporal_fusion: ${use_temporal_fusion}
      use_seperate_decoder: false # TODO
      freeze_backbones: ${freeze_backbones}
      downsample_raw_ele: ${downsample_raw_ele}
      elevation_rescale: ${elevation_rescale}
      encoders:
        camera:
          backbone:
            type: EfficientNetB
            model_name: efficientnet-b0 # b0-b7
            drop_rate: 0.2
            out_indices: [2, 3, 4] # Use 2,3,4 for all efficientNetb0 - b7
            
          neck:
            type: GeneralizedLSSFPN
            in_channels: [40, 112, 320] 
            #b0[40, 112, 320] #b1[40, 112, 320] #b2[48, 120, 352] #b3[48,136,384]
            #b4[56, 160, 448] #b5[64, 176, 512] #b6[72, 200, 576] #b7[80, 224, 640]
            out_channels: 256
            start_level: 0
            num_outs: 3
            norm_cfg:
              type: BN2d
              requires_grad: true
            act_cfg:
              type: ReLU
              inplace: true
            upsample_cfg:
              mode: bilinear
              align_corners: false
          vtransform:
            type: LSSTransform
            in_channels: 256
            out_channels: 80
            image_size: ${image_size}
            # feature_size: ${image_size}//8 #TODO
            feature_downsample: 8
            xbound: ${xbound}
            ybound: ${ybound}
            zbound: ${zbound}
            dbound: ${dbound}
            downsample: 1
        lidar:
          voxelize:
            max_num_points: 16
            point_cloud_range: ${point_cloud_range}
            voxel_size: ${voxel_size}
            max_voxels: [32000, 64000]
          voxelize_reduce: false # Keep this false -> Useful only for 3D Sparse Convs
          backbone:
            type: PointPillarsNet
            voxel_encoder:
              type: PillarFeatureNet
              in_channels: 3 # (x, y, z)
              feat_channels: [64]
              with_distance: False
              voxel_size: ${voxel_size}
              point_cloud_range: ${point_cloud_range}
            middle_encoder:
              type: PointPillarsScatter
              in_channels: 64
              output_shape: ${pointpillar_output_shape} # (x_max-x_min / x_res, y_max-y_min / y_res)
            backbone:
              type: SECOND
              in_channels: 64
              out_channels: [64, 128, 256]
              layer_nums: [3, 5, 5]
              layer_strides: [2, 2, 2]
            neck:
              type: SECONDFPN
              in_channels: [64, 128, 256]
              out_channels: [32, 32, 32]
              upsample_strides: [1, 2, 4]
              use_conv_for_no_stride: true
              upsample_interpolate: ${upsample_interpolate_neck}
        gvom:
          voxelize:
            max_num_points: 16
            point_cloud_range: ${point_cloud_range}
            voxel_size: ${voxel_size}
            max_voxels: [32000, 64000]
          voxelize_reduce: false # Keep this false -> Useful only for 3D Sparse Convs
          backbone:
            type: PointPillarsNet
            voxel_encoder:
              type: PillarFeatureNet
              in_channels: 3 # (x, y, z) -> Change this to 13 when using gvom semantic features
              feat_channels: [64]
              with_distance: False
              voxel_size: ${voxel_size}
              point_cloud_range: ${point_cloud_range}
            middle_encoder:
              type: PointPillarsScatter
              in_channels: 64
              output_shape: ${pointpillar_output_shape} # (x_max-x_min / x_res, y_max-y_min / y_res)
            backbone:
              type: SECOND
              in_channels: 64
              out_channels: [64, 128, 256]
              layer_nums: [3, 5, 5]
              layer_strides: [2, 2, 2]
            neck:
              type: SECONDFPN
              in_channels: [64, 128, 256]
              out_channels: [32, 32, 32]
              upsample_strides: [1, 2, 4]
              use_conv_for_no_stride: true
              upsample_interpolate: ${upsample_interpolate_neck}
      fuser:
        type: ConvFuserUpsample
        in_channels: [80, 96] # This value will get overwritten from the fusion model init
        out_channels: 256
        upsample: ${upsample_after_conv}
      decoder:
        backbone:
          type: BEVDecoderMicro
          in_channels: 256
          mid_channels: 128
          out_channels: 64
          p_dropout: 0.2
        neck:
          type: EmptyNeck
      head:
        type: BEVMultiHead
        in_channels:
          micro: ${model.network.cfg_model.decoder.backbone.out_channels}
        target_layers: ${model.target_layers}

  optimizer:
    _target_: torch.optim.Adam
    _partial_: true
    lr: ${lr}
    weight_decay: 1.0e-7

  scheduler:
    _target_: torch.optim.lr_scheduler.OneCycleLR
    _partial_: true
    max_lr: ${model.optimizer.lr}
    total_steps: ${trainer.max_steps}
    pct_start: 0.1
    div_factor: 25.0
    final_div_factor: 10000.0