{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b50e36-51cc-4acd-b5a5-c357859e771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "from dataclasses import asdict\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from perception_bev_learning.dataset import get_bev_dataloader\n",
    "from perception_bev_learning.cfg import ExperimentParams\n",
    "from perception_bev_learning.lightning import LightningBEV\n",
    "from perception_bev_learning.utils import denormalize_img\n",
    "\n",
    "from dataclasses import asdict\n",
    "import yaml\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e0891-97c3-41ee-95b6-288f4390a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ExperimentParams()\n",
    "cfg.update()\n",
    "module = LightningBEV(cfg)\n",
    "loader_train, loader_val = get_bev_dataloader(cfg, return_test_dataloader=False)\n",
    "\n",
    "for batch in loader_train:\n",
    "    imgs, rots, trans, intrins, post_rots, post_trans, target, aux, *_, pcd_new = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ffab0-60a0-4721-bb08-08d220d32285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just use pretrained efficientnet-b0 to get features maps\n",
    "# Inference manually\n",
    "D = module._model.image_backbone.D\n",
    "C = module._model.image_backbone.camC\n",
    "# downsample = module._model.image_backbone.downsample\n",
    "trunk = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "BS, NR_CAMS = tuple(imgs.shape)[:2]\n",
    "# Flatten BS and NR_CAMs\n",
    "x = imgs.reshape(-1, 3, 256, 384)\n",
    "\n",
    "endpoints = dict()\n",
    "# Stem\n",
    "x = trunk._swish(trunk._bn0(trunk._conv_stem(x)))\n",
    "prev_x = x\n",
    "\n",
    "# Blocks\n",
    "for idx, block in enumerate(trunk._blocks):\n",
    "    drop_connect_rate = trunk._global_params.drop_connect_rate\n",
    "    if drop_connect_rate:\n",
    "        drop_connect_rate *= float(idx) / len(trunk._blocks)  # scale drop connect_rate\n",
    "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "    if prev_x.size(2) > x.size(2):\n",
    "        endpoints[\"reduction_{}\".format(len(endpoints) + 1)] = prev_x\n",
    "    prev_x = x\n",
    "# Head\n",
    "endpoints[\"reduction_{}\".format(len(endpoints) + 1)] = x\n",
    "\n",
    "feat = nn.functional.interpolate(endpoints[\"reduction_5\"], scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "img_features = endpoints[\"reduction_4\"].detach()  # torch.cat( [feat, endpoints[\"reduction_4\"]], dim=1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a962e07-4a83-4e0c-b593-9f7bc416e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features = img_features.reshape(BS, NR_CAMS, -1, 16, 24).reshape(-1, BS, NR_CAMS, 16, 24)\n",
    "np_img_features = img_features[:, :, :, :, :].cpu().numpy().reshape(img_features.shape[0], -1).T\n",
    "n_components = 12\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(np_img_features)\n",
    "pca_comp = pca.transform(np_img_features).reshape(2, 4, 16, 24, n_components)\n",
    "pca_comp_img = pca_comp[0, 0]\n",
    "pca_com_inter = nn.functional.interpolate(\n",
    "    torch.from_numpy(pca_comp_img.T)[None], scale_factor=16, mode=\"bilinear\", align_corners=True\n",
    ").T[:, :, :, 0]\n",
    "pca_com_three = pca_com_inter[:, :, 0:3].numpy()\n",
    "pca_com_three -= pca_com_three.min()\n",
    "pca_com_three /= pca_com_three.max()\n",
    "pca_com_three\n",
    "from PIL import Image\n",
    "\n",
    "Image.fromarray(np.uint8(pca_com_three * 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6637de-e19f-4f7b-a87a-164381ecc5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "denormalize_img(imgs[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937839b5-41b7-4bf2-aae9-aa721785bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from perception_bev_learning.network import BevTravNet\n",
    "\n",
    "path_checkpoint_folder = \"/media/Data/Results/bev_learning/2023-08-28T09-16-59_aux_regress_elevation\"\n",
    "device = \"cpu\"\n",
    "\n",
    "checkpoint = None\n",
    "if checkpoint is None:\n",
    "    checkpoints = [str(s) for s in Path(path_checkpoint_folder).rglob(\"*.ckpt\")]\n",
    "    checkpoints.sort()\n",
    "    checkpoint = checkpoints[-1]\n",
    "ckpt = torch.load(checkpoint)\n",
    "with open(join(path_checkpoint_folder, \"experiment_params.pkl\"), \"rb\") as file:\n",
    "    cfg = pickle.load(file)\n",
    "\n",
    "_cfg = cfg\n",
    "# Initalize Model\n",
    "_model = BevTravNet(cfg.model, cfg.dataset_train)\n",
    "_model.to(device)\n",
    "_model.eval()\n",
    "state_dict = {k.replace(\"_model.\", \"\"): v for k, v in ckpt[\"state_dict\"].items() if \"_model.\" in k}\n",
    "_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ed8e3-11ec-4d43-ab94-5ff0f7b1aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = _model.image_backbone.get_geometry(rots, trans, intrins, post_rots, post_trans)\n",
    "geom.shape\n",
    "\n",
    "x = imgs.reshape(-1, 3, 256, 384)\n",
    "depth, x = _model.image_backbone.camencode.get_depth_feat(x)\n",
    "depth.shape, geom.shape\n",
    "aux[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9efa8bc-6964-4d80-aac0-7f0c085512d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from perception_bev_learning.visu import paper_colors_rgb_f\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "N = 4\n",
    "points = geom\n",
    "co = [torch.tensor(v, device=points.device) for v in paper_colors_rgb_f.values()]\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "vis = points[0].reshape(-1, 3).clone()\n",
    "col = points[0].reshape(-1, 3).clone()\n",
    "delt = int(vis.shape[0] / N)\n",
    "for i in range(N):\n",
    "    col[int(i * delt) : int((i + 1) * delt)] = co[i]\n",
    "pcd.points = o3d.utility.Vector3dVector(vis.cpu().numpy())\n",
    "pcd.colors = o3d.utility.Vector3dVector(col.cpu().numpy())\n",
    "mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=10.0, origin=[0, 0, 0])\n",
    "o3d.visualization.draw_geometries([mesh_frame, pcd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
